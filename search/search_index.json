{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\u041e\u0442\u0447\u0451\u0442 \u0417\u0430\u0434\u0430\u0447\u0430 \u043b\u0430\u0431\u043e\u0440\u0430\u0442\u043e\u0440\u043d\u043e\u0439 \u0440\u0430\u0431\u043e\u0442\u044b \u041d\u0430\u0443\u0447\u0438\u0442\u044c\u0441\u044f \u0443\u043f\u0430\u043a\u043e\u0432\u044b\u0432\u0430\u0442\u044c FastAPI \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u0432 Docker, \u0438\u043d\u0442\u0435\u0433\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043f\u0430\u0440\u0441\u0435\u0440 \u0434\u0430\u043d\u043d\u044b\u0445 \u0441 \u0431\u0430\u0437\u043e\u0439 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u0432\u044b\u0437\u044b\u0432\u0430\u0442\u044c \u043f\u0430\u0440\u0441\u0435\u0440 \u0447\u0435\u0440\u0435\u0437 API \u0438 \u043e\u0447\u0435\u0440\u0435\u0434\u044c. \u0421\u0442\u0435\u043a nginx certbot fastapi postgresql uvicorn gunicorn alembic sqlalchemy jose - \u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f JWT \u0442\u043e\u043a\u0435\u043d\u043e\u0432 redis - \u0434\u043b\u044f \u043e\u0431\u043c\u0435\u043d\u0430 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u044f\u043c\u0438 \u0447\u0435\u0440\u0435\u0437 publisher/subscriber \u0421\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0430 \u043f\u0430\u043f\u043e\u043a \u251c\u2500\u2500 fastapi \u2514\u2500\u2500 src \u2514\u2500\u2500 api \u251c\u2500\u2500 dependecies \u251c\u2500\u2500 routers \u251c\u2500\u2500 transformers \u2514\u2500\u2500 api \u251c\u2500\u2500 config \u2514\u2500\u2500 database \u251c\u2500\u2500 migrations \u2514\u2500\u2500 seeders \u251c\u2500\u2500 utils \u2514\u2500\u2500 env.example \u2514\u2500\u2500 Dockerfile \u2514\u2500\u2500 env.example \u251c\u2500\u2500 nginx \u2514\u2500\u2500 conf.d \u2514\u2500\u2500 app.conf \u251c\u2500\u2500 pgsql \u2514\u2500\u2500 env.example \u2514\u2500\u2500 redis \u2514\u2500\u2500 redis.conf \u251c\u2500\u2500 docker-compose.yml \u2514\u2500\u2500 README.md \u0417\u0430\u043f\u0443\u0441\u043a \u0417\u0430\u043f\u0443\u0441\u043a \u043f\u0440\u043e\u0435\u043a\u0442\u0430 \u043e\u0441\u0443\u0449\u0435\u0441\u0442\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043a\u043e\u043c\u0430\u043d\u0434\u043e\u0439 docker compose up . \u041c\u0438\u0433\u0440\u0430\u0446\u0438\u0438 \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u044e\u0442\u0441\u044f \u0447\u0435\u0440\u0435\u0437 alembic upgrade head \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u041f\u043e\u043b\u0443\u0447\u0438\u043b\u043e\u0441\u044c \u0443\u043f\u0430\u043a\u043e\u0432\u0430\u0442\u044c \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u043d\u0430 fastapi \u0432\u043c\u0435\u0441\u0442\u0435 \u0441 redis \u0438 postgres \u0432 Docker, \u0438\u043d\u0442\u0435\u0433\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0441 \u0411\u0414, \u0440\u0430\u0437\u0432\u0435\u0440\u043d\u0443\u0442\u044c \u0435\u0433\u043e \u0432 \u043e\u0431\u043b\u0430\u043a\u0435 \u0438 \u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0438\u0442\u044c \u0434\u043e\u0441\u0442\u0443\u043f \u043f\u043e https. \u0425\u043e\u0434 \u0440\u0430\u0431\u043e\u0442\u044b Docker \u0420\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u0438 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f: Dockerfile fastapi: \u0443\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u043e\u0432\u0430\u0435\u043c \u0432\u0441\u0435 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0435 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438, \u043e\u0442\u043a\u0440\u044b\u0432\u0430\u0435\u043c \u043f\u043e\u0440\u0442 \u0438 \u0437\u0430\u043f\u0443\u0441\u043a\u0430\u0435\u043c \u0432\u043e\u0440\u043a\u0435\u0440\u044b uvicorn \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e gunicorn FROM python:3.9-alpine WORKDIR /app COPY ./requirements.txt requirements.txt RUN apk update && \\ apk add build-base && \\ pip install -r requirements.txt EXPOSE 8000 CMD [\"gunicorn\", \"src.main:app\", \"--workers\", \"4\", \"--worker-class\", \"uvicorn.workers.UvicornWorker\", \"--bind\", \"0.0.0.0:8000\"] \u0412 \u043a\u043e\u043c\u043f\u043e\u0437-\u0444\u0430\u0439\u043b\u0435 \u043e\u0431\u044a\u044f\u0432\u043b\u0435\u043d\u044b \u043f\u044f\u0442\u044c \u0441\u0435\u0440\u0432\u0438\u0441\u043e\u0432: \u041f\u0440\u043e\u043a\u0441\u0438 \u0441\u0435\u0440\u0432\u0435\u0440 (nginx) \u0441\u0435\u0440\u0442\u0431\u043e\u0442 \u0434\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f ssl-\u0441\u0435\u0440\u0442\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0432 (certbot) \u0411\u0430\u0437\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 (postgres) Redis (redis) \u041f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435 API (fastapi) ```YML title=\"docker-compose.yml\" version: '3' services: nginx: image: nginx:alpine container_name: otiva_nginx restart: unless-stopped ports: - \"80:80\" - \"443:443\" volumes: - ./certbot/www:/var/www/certbot/ - ./certbot/conf:/etc/letsencrypt/ - ./nginx/conf.d/:/etc/nginx/conf.d/ networks: - otiva fastapi: container_name: otiva_fastapi restart: unless-stopped build: context: ./fastapi dockerfile: Dockerfile ports: - \"5000:5000\" volumes: - ./fastapi:/app networks: - otiva postgres: container_name: otiva_postgres restart: unless-stopped image: postgres:14 ports: - \"5432:5432\" env_file: - ./pgsql/.env volumes: - otiva-pgsql-volume:/var/lib/postgresql/data networks: - otiva redis: image: redis:alpine container_name: otiva_redis restart: unless-stopped volumes: - otiva-redis-volume:/data - ./redis/redis.conf:/usr/local/etc/redis/redis.conf expose: - 6379 command: redis-server /usr/local/etc/redis/redis.conf networks: - otiva certbot: image: certbot/certbot container_name: otiva_certbot volumes: - ./certbot/conf:/etc/letsencrypt - ./certbot/www:/var/www/certbot command: certonly --webroot -w /var/www/certbot/ --email test.space@gmail.com --agree-tos networks: otiva: driver: bridge volumes: otiva-pgsql-volume: driver: local otiva-redis-volume: driver: local \u041a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u044f nginx ```bash title=app.conf server { server { listen 80; server_name api.otiva.space; location /.well-known/acme-challenge/ { root /var/www/certbot; } location / { return 301 https://$host$request_uri; } } server { listen 443 ssl; server_name api.otiva.space; ssl_certificate /etc/letsencrypt/live/otiva/fullchain.pem; # managed by Certbot ssl_certificate_key /etc/letsencrypt/live/otiva/privkey.pem; # managed by Certbot charset utf-8; set $cors_origin \"\"; set $cors_cred \"\"; set $cors_header \"\"; set $cors_method \"\"; if ($http_origin = \"https://otiva.space\") { set $cors_origin $http_origin; set $cors_cred true; set $cors_header $http_access_control_request_headers; set $cors_method $http_access_control_request_method; } add_header Access-Control-Allow-Origin $http_origin always; add_header Access-Control-Allow-Credentials $cors_cred always; add_header Access-Control-Allow-Headers $cors_header always; add_header Access-Control-Allow-Methods $cors_method always; if ($request_method = 'OPTIONS') { return 204; } location /ws { proxy_pass http://otiva_fastapi:8000; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"Upgrade\"; proxy_set_header Host $host; } location / { proxy_pass http://otiva_fastapi:8000; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_redirect off; } } FastAPI \u0411\u044b\u043b \u043d\u0430\u043f\u0438\u0441\u0430\u043d \u043d\u0435\u0431\u043e\u043b\u044c\u0448\u043e\u0439 \u043a\u043b\u0438\u0435\u043d\u0442 \u0434\u043b\u044f \u0440\u0430\u0431\u043e\u0442\u044b \u0441 redis pub/sub ```python title=pubsub.py from src.utils.redis import redis import json import asyncio from typing import Union class PubSubEvents: NEW_MESSAGE: str = 'new_message' class PubSubMessage: def init (self, data, pattern, channel, type): self.data = data self.pattern = pattern self.channel = channel self.type = type class PubSubEvent: def init (self, name, data: Union[list, dict]): self.name = name self.data = data def serialize(self) -> str: data = { 'name': self.name, 'data': self.data, } return json.dumps(data) class PubSub: @staticmethod async def publish(payload: dict[str, PubSubEvent]): \"\"\"Accepts dict {channel_name: data} and publishes data to respective channel names\"\"\" if not payload: return pub = redis.get_connection() for channel in payload: event: PubSubEvent = payload[channel] await pub.publish(channel, event.serialize()) await pub.close() @staticmethod async def subscribe(channel: str): sub = redis.get_connection().pubsub() async with sub as conn: await conn.subscribe(channel) while True: try: message = await conn.get_message(ignore_subscribe_messages=True) if message is not None: dto = PubSubMessage( data=message['data'].decode('utf-8'), pattern=message['pattern'].decode('utf-8') if message['pattern'] else None, channel=message['channel'].decode('utf-8') if message['channel'] else None, type=message['type'] if message['type'] else None, ) yield dto await asyncio.sleep(1) except Exception: break await conn.unsubscribe(channel) await sub.close() Pub/sub redis \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0432 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0438 \u0434\u043b\u044f \u043e\u0442\u043f\u0440\u0430\u0432\u043a\u0438 \u0443\u0432\u0435\u0434\u043e\u043c\u043b\u0435\u043d\u0438\u0439 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044e \u0447\u0435\u0440\u0435\u0437\u0437 Websockets ```python title=notifications.py from fastapi import WebSocket, APIRouter, Depends from src.api.dependencies.auth import Auth from src.utils.pubsub import PubSub router = APIRouter() @router.websocket(\"/ws/notifications\") async def websocket_endpoint(websocket: WebSocket, auth: Auth = Depends()): await websocket.accept() try: user = await auth.check_access_token_websocket(websocket) except Exception as e: await websocket.send_text('Unauthenticated') await websocket.close() return channel_name = f'notifications:{user.id}' async for message in PubSub.subscribe(channel_name): await websocket.send_text(message.data) await websocket.close() \u041f\u0440\u0438\u043c\u0435\u0440 \u043e\u0442\u043f\u0440\u0430\u0432\u043a\u0438 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u044f \u043e \u043d\u043e\u0432\u043e\u043c \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u0438 \u0432 \u0447\u0430\u0442\u0435: ```python title=chat.py @router.post('/{chat_id}/messages') async def store_message(chat_id: int, request: Request, auth: Auth = Depends()): await auth.check_access_token(request) validator = Validator(await request.json() | {'chat_id': chat_id}, { 'text': ['required', 'string'], }, {}, BasePayload()) payload = validator.validated() # chat must have current user as ChatUser async with db_manager.get_session() as session: q = select(Chat) \\ .options(joinedload(Chat.chat_users)) \\ .where(Chat.id == chat_id) res = await session.execute(q) chat: Chat = res.scalar() if not chat: return ApiResponse.error('Chat does not exists.') current_chat_user: ChatUser = next(filter(lambda cu: cu.user_id == request.state.user.id, chat.chat_users), None) if not current_chat_user: return ApiResponse.error('User does not belongs to this chat.') message: ChatMessage = await SqlAlchemyRepository(db_manager.get_session, ChatMessage).create({ 'chat_user_id': current_chat_user.id, 'chat_id': chat_id, 'text': payload.text, 'seen_at': None, }) transformed_message = transform(message, ChatMessageTransformer()) notifications = {} for chat_user in chat.chat_users: # do not notify sender if chat_user.user_id == request.state.user.id: continue notifications[f'notifications:{chat_user.user_id}'] = PubSubEvent(PubSubEvents.NEW_MESSAGE, transformed_message) await PubSub.publish(notifications) # todo to queue return ApiResponse.payload(transformed_message) ``` \u041f\u0440\u0438\u043c\u0435\u0440 \u0440\u0430\u0431\u043e\u0442\u044b: \u041f\u043e\u0434\u043a\u043b\u044e\u0447\u0430\u0435\u043c\u0441\u044f \u043a \u0432\u0435\u0431\u0441\u043e\u043a\u0435\u0442\u0443 \u041e\u0442\u043f\u0440\u0430\u0432\u043b\u044f\u0435\u043c \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u0435 \u0423\u0432\u0435\u0434\u043e\u043c\u043b\u0435\u043d\u0438\u0435 \u043f\u0440\u0438\u0448\u043b\u043e","title":"L3"},{"location":"#_1","text":"","title":"\u041e\u0442\u0447\u0451\u0442"},{"location":"#_2","text":"\u041d\u0430\u0443\u0447\u0438\u0442\u044c\u0441\u044f \u0443\u043f\u0430\u043a\u043e\u0432\u044b\u0432\u0430\u0442\u044c FastAPI \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u0432 Docker, \u0438\u043d\u0442\u0435\u0433\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043f\u0430\u0440\u0441\u0435\u0440 \u0434\u0430\u043d\u043d\u044b\u0445 \u0441 \u0431\u0430\u0437\u043e\u0439 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u0432\u044b\u0437\u044b\u0432\u0430\u0442\u044c \u043f\u0430\u0440\u0441\u0435\u0440 \u0447\u0435\u0440\u0435\u0437 API \u0438 \u043e\u0447\u0435\u0440\u0435\u0434\u044c.","title":"\u0417\u0430\u0434\u0430\u0447\u0430 \u043b\u0430\u0431\u043e\u0440\u0430\u0442\u043e\u0440\u043d\u043e\u0439 \u0440\u0430\u0431\u043e\u0442\u044b"},{"location":"#_3","text":"nginx certbot fastapi postgresql uvicorn gunicorn alembic sqlalchemy jose - \u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f JWT \u0442\u043e\u043a\u0435\u043d\u043e\u0432 redis - \u0434\u043b\u044f \u043e\u0431\u043c\u0435\u043d\u0430 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u044f\u043c\u0438 \u0447\u0435\u0440\u0435\u0437 publisher/subscriber","title":"\u0421\u0442\u0435\u043a"},{"location":"#_4","text":"\u251c\u2500\u2500 fastapi \u2514\u2500\u2500 src \u2514\u2500\u2500 api \u251c\u2500\u2500 dependecies \u251c\u2500\u2500 routers \u251c\u2500\u2500 transformers \u2514\u2500\u2500 api \u251c\u2500\u2500 config \u2514\u2500\u2500 database \u251c\u2500\u2500 migrations \u2514\u2500\u2500 seeders \u251c\u2500\u2500 utils \u2514\u2500\u2500 env.example \u2514\u2500\u2500 Dockerfile \u2514\u2500\u2500 env.example \u251c\u2500\u2500 nginx \u2514\u2500\u2500 conf.d \u2514\u2500\u2500 app.conf \u251c\u2500\u2500 pgsql \u2514\u2500\u2500 env.example \u2514\u2500\u2500 redis \u2514\u2500\u2500 redis.conf \u251c\u2500\u2500 docker-compose.yml \u2514\u2500\u2500 README.md","title":"\u0421\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0430 \u043f\u0430\u043f\u043e\u043a"},{"location":"#_5","text":"\u0417\u0430\u043f\u0443\u0441\u043a \u043f\u0440\u043e\u0435\u043a\u0442\u0430 \u043e\u0441\u0443\u0449\u0435\u0441\u0442\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043a\u043e\u043c\u0430\u043d\u0434\u043e\u0439 docker compose up . \u041c\u0438\u0433\u0440\u0430\u0446\u0438\u0438 \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u044e\u0442\u0441\u044f \u0447\u0435\u0440\u0435\u0437 alembic upgrade head","title":"\u0417\u0430\u043f\u0443\u0441\u043a"},{"location":"#_6","text":"\u041f\u043e\u043b\u0443\u0447\u0438\u043b\u043e\u0441\u044c \u0443\u043f\u0430\u043a\u043e\u0432\u0430\u0442\u044c \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u043d\u0430 fastapi \u0432\u043c\u0435\u0441\u0442\u0435 \u0441 redis \u0438 postgres \u0432 Docker, \u0438\u043d\u0442\u0435\u0433\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0441 \u0411\u0414, \u0440\u0430\u0437\u0432\u0435\u0440\u043d\u0443\u0442\u044c \u0435\u0433\u043e \u0432 \u043e\u0431\u043b\u0430\u043a\u0435 \u0438 \u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0438\u0442\u044c \u0434\u043e\u0441\u0442\u0443\u043f \u043f\u043e https.","title":"\u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b"},{"location":"#_7","text":"","title":"\u0425\u043e\u0434 \u0440\u0430\u0431\u043e\u0442\u044b"},{"location":"#docker","text":"\u0420\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u0438 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u044f: Dockerfile fastapi: \u0443\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u043e\u0432\u0430\u0435\u043c \u0432\u0441\u0435 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0435 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438, \u043e\u0442\u043a\u0440\u044b\u0432\u0430\u0435\u043c \u043f\u043e\u0440\u0442 \u0438 \u0437\u0430\u043f\u0443\u0441\u043a\u0430\u0435\u043c \u0432\u043e\u0440\u043a\u0435\u0440\u044b uvicorn \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e gunicorn FROM python:3.9-alpine WORKDIR /app COPY ./requirements.txt requirements.txt RUN apk update && \\ apk add build-base && \\ pip install -r requirements.txt EXPOSE 8000 CMD [\"gunicorn\", \"src.main:app\", \"--workers\", \"4\", \"--worker-class\", \"uvicorn.workers.UvicornWorker\", \"--bind\", \"0.0.0.0:8000\"] \u0412 \u043a\u043e\u043c\u043f\u043e\u0437-\u0444\u0430\u0439\u043b\u0435 \u043e\u0431\u044a\u044f\u0432\u043b\u0435\u043d\u044b \u043f\u044f\u0442\u044c \u0441\u0435\u0440\u0432\u0438\u0441\u043e\u0432: \u041f\u0440\u043e\u043a\u0441\u0438 \u0441\u0435\u0440\u0432\u0435\u0440 (nginx) \u0441\u0435\u0440\u0442\u0431\u043e\u0442 \u0434\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f ssl-\u0441\u0435\u0440\u0442\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0432 (certbot) \u0411\u0430\u0437\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 (postgres) Redis (redis) \u041f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435 API (fastapi) ```YML title=\"docker-compose.yml\" version: '3' services: nginx: image: nginx:alpine container_name: otiva_nginx restart: unless-stopped ports: - \"80:80\" - \"443:443\" volumes: - ./certbot/www:/var/www/certbot/ - ./certbot/conf:/etc/letsencrypt/ - ./nginx/conf.d/:/etc/nginx/conf.d/ networks: - otiva fastapi: container_name: otiva_fastapi restart: unless-stopped build: context: ./fastapi dockerfile: Dockerfile ports: - \"5000:5000\" volumes: - ./fastapi:/app networks: - otiva postgres: container_name: otiva_postgres restart: unless-stopped image: postgres:14 ports: - \"5432:5432\" env_file: - ./pgsql/.env volumes: - otiva-pgsql-volume:/var/lib/postgresql/data networks: - otiva redis: image: redis:alpine container_name: otiva_redis restart: unless-stopped volumes: - otiva-redis-volume:/data - ./redis/redis.conf:/usr/local/etc/redis/redis.conf expose: - 6379 command: redis-server /usr/local/etc/redis/redis.conf networks: - otiva certbot: image: certbot/certbot container_name: otiva_certbot volumes: - ./certbot/conf:/etc/letsencrypt - ./certbot/www:/var/www/certbot command: certonly --webroot -w /var/www/certbot/ --email test.space@gmail.com --agree-tos networks: otiva: driver: bridge volumes: otiva-pgsql-volume: driver: local otiva-redis-volume: driver: local \u041a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u044f nginx ```bash title=app.conf server { server { listen 80; server_name api.otiva.space; location /.well-known/acme-challenge/ { root /var/www/certbot; } location / { return 301 https://$host$request_uri; } } server { listen 443 ssl; server_name api.otiva.space; ssl_certificate /etc/letsencrypt/live/otiva/fullchain.pem; # managed by Certbot ssl_certificate_key /etc/letsencrypt/live/otiva/privkey.pem; # managed by Certbot charset utf-8; set $cors_origin \"\"; set $cors_cred \"\"; set $cors_header \"\"; set $cors_method \"\"; if ($http_origin = \"https://otiva.space\") { set $cors_origin $http_origin; set $cors_cred true; set $cors_header $http_access_control_request_headers; set $cors_method $http_access_control_request_method; } add_header Access-Control-Allow-Origin $http_origin always; add_header Access-Control-Allow-Credentials $cors_cred always; add_header Access-Control-Allow-Headers $cors_header always; add_header Access-Control-Allow-Methods $cors_method always; if ($request_method = 'OPTIONS') { return 204; } location /ws { proxy_pass http://otiva_fastapi:8000; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"Upgrade\"; proxy_set_header Host $host; } location / { proxy_pass http://otiva_fastapi:8000; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_redirect off; } }","title":"Docker"},{"location":"#fastapi","text":"\u0411\u044b\u043b \u043d\u0430\u043f\u0438\u0441\u0430\u043d \u043d\u0435\u0431\u043e\u043b\u044c\u0448\u043e\u0439 \u043a\u043b\u0438\u0435\u043d\u0442 \u0434\u043b\u044f \u0440\u0430\u0431\u043e\u0442\u044b \u0441 redis pub/sub ```python title=pubsub.py from src.utils.redis import redis import json import asyncio from typing import Union class PubSubEvents: NEW_MESSAGE: str = 'new_message' class PubSubMessage: def init (self, data, pattern, channel, type): self.data = data self.pattern = pattern self.channel = channel self.type = type class PubSubEvent: def init (self, name, data: Union[list, dict]): self.name = name self.data = data def serialize(self) -> str: data = { 'name': self.name, 'data': self.data, } return json.dumps(data) class PubSub: @staticmethod async def publish(payload: dict[str, PubSubEvent]): \"\"\"Accepts dict {channel_name: data} and publishes data to respective channel names\"\"\" if not payload: return pub = redis.get_connection() for channel in payload: event: PubSubEvent = payload[channel] await pub.publish(channel, event.serialize()) await pub.close() @staticmethod async def subscribe(channel: str): sub = redis.get_connection().pubsub() async with sub as conn: await conn.subscribe(channel) while True: try: message = await conn.get_message(ignore_subscribe_messages=True) if message is not None: dto = PubSubMessage( data=message['data'].decode('utf-8'), pattern=message['pattern'].decode('utf-8') if message['pattern'] else None, channel=message['channel'].decode('utf-8') if message['channel'] else None, type=message['type'] if message['type'] else None, ) yield dto await asyncio.sleep(1) except Exception: break await conn.unsubscribe(channel) await sub.close() Pub/sub redis \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0432 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0438 \u0434\u043b\u044f \u043e\u0442\u043f\u0440\u0430\u0432\u043a\u0438 \u0443\u0432\u0435\u0434\u043e\u043c\u043b\u0435\u043d\u0438\u0439 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044e \u0447\u0435\u0440\u0435\u0437\u0437 Websockets ```python title=notifications.py from fastapi import WebSocket, APIRouter, Depends from src.api.dependencies.auth import Auth from src.utils.pubsub import PubSub router = APIRouter() @router.websocket(\"/ws/notifications\") async def websocket_endpoint(websocket: WebSocket, auth: Auth = Depends()): await websocket.accept() try: user = await auth.check_access_token_websocket(websocket) except Exception as e: await websocket.send_text('Unauthenticated') await websocket.close() return channel_name = f'notifications:{user.id}' async for message in PubSub.subscribe(channel_name): await websocket.send_text(message.data) await websocket.close() \u041f\u0440\u0438\u043c\u0435\u0440 \u043e\u0442\u043f\u0440\u0430\u0432\u043a\u0438 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u044f \u043e \u043d\u043e\u0432\u043e\u043c \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u0438 \u0432 \u0447\u0430\u0442\u0435: ```python title=chat.py @router.post('/{chat_id}/messages') async def store_message(chat_id: int, request: Request, auth: Auth = Depends()): await auth.check_access_token(request) validator = Validator(await request.json() | {'chat_id': chat_id}, { 'text': ['required', 'string'], }, {}, BasePayload()) payload = validator.validated() # chat must have current user as ChatUser async with db_manager.get_session() as session: q = select(Chat) \\ .options(joinedload(Chat.chat_users)) \\ .where(Chat.id == chat_id) res = await session.execute(q) chat: Chat = res.scalar() if not chat: return ApiResponse.error('Chat does not exists.') current_chat_user: ChatUser = next(filter(lambda cu: cu.user_id == request.state.user.id, chat.chat_users), None) if not current_chat_user: return ApiResponse.error('User does not belongs to this chat.') message: ChatMessage = await SqlAlchemyRepository(db_manager.get_session, ChatMessage).create({ 'chat_user_id': current_chat_user.id, 'chat_id': chat_id, 'text': payload.text, 'seen_at': None, }) transformed_message = transform(message, ChatMessageTransformer()) notifications = {} for chat_user in chat.chat_users: # do not notify sender if chat_user.user_id == request.state.user.id: continue notifications[f'notifications:{chat_user.user_id}'] = PubSubEvent(PubSubEvents.NEW_MESSAGE, transformed_message) await PubSub.publish(notifications) # todo to queue return ApiResponse.payload(transformed_message) ```","title":"FastAPI"},{"location":"#_8","text":"","title":"\u041f\u0440\u0438\u043c\u0435\u0440 \u0440\u0430\u0431\u043e\u0442\u044b:"},{"location":"#_9","text":"","title":"\u041f\u043e\u0434\u043a\u043b\u044e\u0447\u0430\u0435\u043c\u0441\u044f \u043a \u0432\u0435\u0431\u0441\u043e\u043a\u0435\u0442\u0443"},{"location":"#_10","text":"","title":"\u041e\u0442\u043f\u0440\u0430\u0432\u043b\u044f\u0435\u043c \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u0435"},{"location":"#_11","text":"","title":"\u0423\u0432\u0435\u0434\u043e\u043c\u043b\u0435\u043d\u0438\u0435 \u043f\u0440\u0438\u0448\u043b\u043e"},{"location":"task1/","text":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 1 \u0421 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0442\u0440\u0435\u0445 \u043f\u043e\u0434\u0445\u043e\u0434\u043e\u0432: threading, multiprocessing \u0438 async/await \u0441\u0443\u043c\u043c\u0430 \u0447\u0438\u0441\u0435\u043b \u043e\u0442 1 \u0434\u043e 1000000 \u0431\u044b\u043b\u0430 \u0440\u0430\u0437\u0431\u0438\u0442\u0430 \u043d\u0430 \u0437\u0430\u0440\u0430\u043d\u0435\u0435 \u0437\u0430\u0434\u0434\u0430\u043d\u043d\u043e\u0435 \u0447\u0438\u0441\u043b\u043e \u0447\u0430\u043d\u043a\u043e\u0432 \u0438 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0430 sum_threading.py from multiprocessing.pool import ThreadPool from time import time RUNS_NUMBER = 3 CHUNKS_NUMBER = 4 NUMBER = 1000000 def calc(lr): l, r = lr return sum(range(l, r)) def main(): args = [] for i in range(CHUNKS_NUMBER): l = int(i * NUMBER / CHUNKS_NUMBER + 1) r = int(l + NUMBER / CHUNKS_NUMBER) args.append((l, r)) pool = ThreadPool(len(args)) start = time() res = pool.map(calc, args) pool.close() pool.join() exec_time = time() - start return exec_time, res if __name__ == \"__main__\": times = [] for i in range(RUNS_NUMBER): next_time, res = main() times.append(next_time) print(i + 1, ' run: ', next_time, 's. RESULT: ', sum(res)) print('Average time: ', sum(times) / len(times), 's') multiprocessing_sum.py import multiprocessing from time import time RUNS_NUMBER = 3 CHUNKS_NUMBER = 4 NUMBER = 1000000 def calc(lr): l, r = lr return sum(range(l, r)) def main(): args = [] for i in range(CHUNKS_NUMBER): l = int(i * NUMBER / CHUNKS_NUMBER + 1) r = int(l + NUMBER / CHUNKS_NUMBER) args.append((l, r)) pool = multiprocessing.Pool(len(args)) start = time() res = pool.map(calc, args) pool.close() pool.join() exec_time = time() - start return exec_time, res if __name__ == \"__main__\": times = [] for i in range(RUNS_NUMBER): next_time, res = main() times.append(next_time) print(i + 1, ' run: ', next_time, 's. RESULT: ', sum(res)) print('Average time: ', sum(times) / len(times), 's') asyncio_sum.py import asyncio from time import time RUNS_NUMBER = 3 CHUNKS_NUMBER = 4 NUMBER = 1000000 async def calc(l, r): return sum(range(l, r)) async def main(): tasks = [] for i in range(CHUNKS_NUMBER): l = int(i * NUMBER / CHUNKS_NUMBER + 1) r = int(l + NUMBER / CHUNKS_NUMBER) task = asyncio.create_task(calc(l, r)) tasks.append(task) start = time() res = await asyncio.gather(*tasks) exec_time = time() - start return exec_time, res if __name__ == \"__main__\": times = [] for i in range(RUNS_NUMBER): next_time, res = asyncio.run(main()) times.append(next_time) print(i + 1, ' run: ', next_time, 's. RESULT: ', sum(res)) print('Average time: ', sum(times) / len(times), 's') \u0412\u044b\u0432\u043e\u0434\u044b Threading Multiprocessing Asyncio 1 0.029 0.16 0.027 2 0.030 0.14 0.028 3 0.029 0.12 0.027 Avg 0.029 0.14 0.0273 Threading : \u0412\u0440\u0435\u043c\u044f \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f \u043e\u0434\u043d\u043e\u0433\u043e \u043f\u043e\u0440\u044f\u0434\u043a\u0430 \u0441 asyncio. \u041e\u0434\u043d\u0430\u043a\u043e, Threading \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d GIL, \u0447\u0442\u043e \u043c\u043e\u0436\u0435\u0442 \u0437\u0430\u043c\u0435\u0434\u043b\u0438\u0442\u044c \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u0435 \u043d\u0430 \u043c\u043d\u043e\u0433\u043e\u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0440\u043d\u044b\u0445 \u0441\u0438\u0441\u0442\u0435\u043c\u0430\u0445. Multiprocessing : \u041f\u0440\u043e\u0438\u0433\u0440\u044b\u0432\u0430\u0435\u0442 \u043d\u0430 \u043f\u043e\u0440\u044f\u0434\u043e\u043a \u043e\u0441\u0442\u0430\u043b\u044c\u043d\u044b\u043c \u043c\u0435\u0442\u043e\u0434\u0430\u043c. \u0421\u043a\u043e\u0440\u0435\u0435 \u0432\u0441\u0435\u0433\u043e \u0432\u0440\u0435\u043c\u044f \u0443\u0432\u0435\u043b\u0438\u0447\u0435\u043d\u043e \u0438\u0437-\u0437\u0430 \u0440\u0435\u0441\u0443\u0440\u0441\u043d\u044b\u0445 \u0437\u0430\u0442\u0440\u0430\u0442 \u043d\u0430 \u043f\u0435\u0440\u0435\u043a\u043b\u044e\u0447\u0447\u0435\u043d\u0438\u0435 \u043c\u0435\u0436\u0434\u0443 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u0430\u043c\u0438. Asyncio : \u041e\u0431\u0435\u0441\u043f\u0435\u0447\u0438\u0432\u0430\u0435\u0442 \u043b\u0443\u0447\u0448\u0443\u044e \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u0431\u043b\u0430\u0433\u043e\u0434\u0430\u0440\u044f \u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u043e\u043c\u0443 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044e \u0430\u0441\u0438\u043d\u0445\u0440\u043e\u043d\u043d\u044b\u0445 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0439 \u0431\u0435\u0437 \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0432 \u0438\u043b\u0438 \u043f\u043e\u0442\u043e\u043a\u043e\u0432. \u041f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435 Threading Multiprocessing Asyncio 1 28.88 11.23 28.44 2 28.90 10.67 29.03 3 29.68 10.90 34.16 Avg 29.15 10.93 30.54 \u041d\u0430 1 000 000 000 multiprocessing \u043f\u043e\u043a\u0430\u0437\u0430\u043b \u043d\u0430\u0438\u043b\u0443\u0447\u0448\u0438\u0439 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442. \u041e\u0442\u0441\u044e\u0434\u0430 \u043c\u043e\u0436\u043d\u043e \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u0432\u044b\u0432\u043e\u0434, \u0447\u0442\u043e \u0441 \u0440\u043e\u0441\u0442\u043e\u043c \u0441\u043b\u043e\u0436\u043d\u043e\u0441\u0442\u0438 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0439 \u0437\u0430\u0442\u0440\u0430\u0442\u044b \u043d\u0430 \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0438 \u0443\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u0430\u043c\u0438 \u043d\u0438\u0432\u0435\u043b\u0438\u0440\u0443\u0435\u0442\u0441\u044f.","title":"L2.1"},{"location":"task1/#1","text":"\u0421 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0442\u0440\u0435\u0445 \u043f\u043e\u0434\u0445\u043e\u0434\u043e\u0432: threading, multiprocessing \u0438 async/await \u0441\u0443\u043c\u043c\u0430 \u0447\u0438\u0441\u0435\u043b \u043e\u0442 1 \u0434\u043e 1000000 \u0431\u044b\u043b\u0430 \u0440\u0430\u0437\u0431\u0438\u0442\u0430 \u043d\u0430 \u0437\u0430\u0440\u0430\u043d\u0435\u0435 \u0437\u0430\u0434\u0434\u0430\u043d\u043d\u043e\u0435 \u0447\u0438\u0441\u043b\u043e \u0447\u0430\u043d\u043a\u043e\u0432 \u0438 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0430","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 1"},{"location":"task1/#sum_threadingpy","text":"from multiprocessing.pool import ThreadPool from time import time RUNS_NUMBER = 3 CHUNKS_NUMBER = 4 NUMBER = 1000000 def calc(lr): l, r = lr return sum(range(l, r)) def main(): args = [] for i in range(CHUNKS_NUMBER): l = int(i * NUMBER / CHUNKS_NUMBER + 1) r = int(l + NUMBER / CHUNKS_NUMBER) args.append((l, r)) pool = ThreadPool(len(args)) start = time() res = pool.map(calc, args) pool.close() pool.join() exec_time = time() - start return exec_time, res if __name__ == \"__main__\": times = [] for i in range(RUNS_NUMBER): next_time, res = main() times.append(next_time) print(i + 1, ' run: ', next_time, 's. RESULT: ', sum(res)) print('Average time: ', sum(times) / len(times), 's')","title":"sum_threading.py"},{"location":"task1/#multiprocessing_sumpy","text":"import multiprocessing from time import time RUNS_NUMBER = 3 CHUNKS_NUMBER = 4 NUMBER = 1000000 def calc(lr): l, r = lr return sum(range(l, r)) def main(): args = [] for i in range(CHUNKS_NUMBER): l = int(i * NUMBER / CHUNKS_NUMBER + 1) r = int(l + NUMBER / CHUNKS_NUMBER) args.append((l, r)) pool = multiprocessing.Pool(len(args)) start = time() res = pool.map(calc, args) pool.close() pool.join() exec_time = time() - start return exec_time, res if __name__ == \"__main__\": times = [] for i in range(RUNS_NUMBER): next_time, res = main() times.append(next_time) print(i + 1, ' run: ', next_time, 's. RESULT: ', sum(res)) print('Average time: ', sum(times) / len(times), 's')","title":"multiprocessing_sum.py"},{"location":"task1/#asyncio_sumpy","text":"import asyncio from time import time RUNS_NUMBER = 3 CHUNKS_NUMBER = 4 NUMBER = 1000000 async def calc(l, r): return sum(range(l, r)) async def main(): tasks = [] for i in range(CHUNKS_NUMBER): l = int(i * NUMBER / CHUNKS_NUMBER + 1) r = int(l + NUMBER / CHUNKS_NUMBER) task = asyncio.create_task(calc(l, r)) tasks.append(task) start = time() res = await asyncio.gather(*tasks) exec_time = time() - start return exec_time, res if __name__ == \"__main__\": times = [] for i in range(RUNS_NUMBER): next_time, res = asyncio.run(main()) times.append(next_time) print(i + 1, ' run: ', next_time, 's. RESULT: ', sum(res)) print('Average time: ', sum(times) / len(times), 's')","title":"asyncio_sum.py"},{"location":"task1/#_1","text":"Threading Multiprocessing Asyncio 1 0.029 0.16 0.027 2 0.030 0.14 0.028 3 0.029 0.12 0.027 Avg 0.029 0.14 0.0273 Threading : \u0412\u0440\u0435\u043c\u044f \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f \u043e\u0434\u043d\u043e\u0433\u043e \u043f\u043e\u0440\u044f\u0434\u043a\u0430 \u0441 asyncio. \u041e\u0434\u043d\u0430\u043a\u043e, Threading \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d GIL, \u0447\u0442\u043e \u043c\u043e\u0436\u0435\u0442 \u0437\u0430\u043c\u0435\u0434\u043b\u0438\u0442\u044c \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u0435 \u043d\u0430 \u043c\u043d\u043e\u0433\u043e\u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0440\u043d\u044b\u0445 \u0441\u0438\u0441\u0442\u0435\u043c\u0430\u0445. Multiprocessing : \u041f\u0440\u043e\u0438\u0433\u0440\u044b\u0432\u0430\u0435\u0442 \u043d\u0430 \u043f\u043e\u0440\u044f\u0434\u043e\u043a \u043e\u0441\u0442\u0430\u043b\u044c\u043d\u044b\u043c \u043c\u0435\u0442\u043e\u0434\u0430\u043c. \u0421\u043a\u043e\u0440\u0435\u0435 \u0432\u0441\u0435\u0433\u043e \u0432\u0440\u0435\u043c\u044f \u0443\u0432\u0435\u043b\u0438\u0447\u0435\u043d\u043e \u0438\u0437-\u0437\u0430 \u0440\u0435\u0441\u0443\u0440\u0441\u043d\u044b\u0445 \u0437\u0430\u0442\u0440\u0430\u0442 \u043d\u0430 \u043f\u0435\u0440\u0435\u043a\u043b\u044e\u0447\u0447\u0435\u043d\u0438\u0435 \u043c\u0435\u0436\u0434\u0443 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u0430\u043c\u0438. Asyncio : \u041e\u0431\u0435\u0441\u043f\u0435\u0447\u0438\u0432\u0430\u0435\u0442 \u043b\u0443\u0447\u0448\u0443\u044e \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u0431\u043b\u0430\u0433\u043e\u0434\u0430\u0440\u044f \u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u043e\u043c\u0443 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044e \u0430\u0441\u0438\u043d\u0445\u0440\u043e\u043d\u043d\u044b\u0445 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0439 \u0431\u0435\u0437 \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0432 \u0438\u043b\u0438 \u043f\u043e\u0442\u043e\u043a\u043e\u0432.","title":"\u0412\u044b\u0432\u043e\u0434\u044b"},{"location":"task1/#_2","text":"Threading Multiprocessing Asyncio 1 28.88 11.23 28.44 2 28.90 10.67 29.03 3 29.68 10.90 34.16 Avg 29.15 10.93 30.54 \u041d\u0430 1 000 000 000 multiprocessing \u043f\u043e\u043a\u0430\u0437\u0430\u043b \u043d\u0430\u0438\u043b\u0443\u0447\u0448\u0438\u0439 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442. \u041e\u0442\u0441\u044e\u0434\u0430 \u043c\u043e\u0436\u043d\u043e \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u0432\u044b\u0432\u043e\u0434, \u0447\u0442\u043e \u0441 \u0440\u043e\u0441\u0442\u043e\u043c \u0441\u043b\u043e\u0436\u043d\u043e\u0441\u0442\u0438 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0439 \u0437\u0430\u0442\u0440\u0430\u0442\u044b \u043d\u0430 \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0438 \u0443\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u0430\u043c\u0438 \u043d\u0438\u0432\u0435\u043b\u0438\u0440\u0443\u0435\u0442\u0441\u044f.","title":"\u041f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435"},{"location":"task2/","text":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 2 \u041d\u0430\u043f\u0438\u0448\u0438\u0442\u0435 \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0443 \u043d\u0430 Python \u0434\u043b\u044f \u043f\u0430\u0440\u0430\u043b\u043b\u0435\u043b\u044c\u043d\u043e\u0433\u043e \u043f\u0430\u0440\u0441\u0438\u043d\u0433\u0430 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u0445 \u0432\u0435\u0431-\u0441\u0442\u0440\u0430\u043d\u0438\u0446 \u0441 \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0445 \u0432 \u0431\u0430\u0437\u0443 \u0434\u0430\u043d\u043d\u044b\u0445 \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u043f\u043e\u0434\u0445\u043e\u0434\u043e\u0432 threading, multiprocessing \u0438 async. \u041a\u0430\u0436\u0434\u0430\u044f \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0430 \u0434\u043e\u043b\u0436\u043d\u0430 \u043f\u0430\u0440\u0441\u0438\u0442\u044c \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u0441 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u0445 \u0432\u0435\u0431-\u0441\u0430\u0439\u0442\u043e\u0432, \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0442\u044c \u0438\u0445 \u0432 \u0431\u0430\u0437\u0443 \u0434\u0430\u043d\u043d\u044b\u0445. parse_threading.py from multiprocessing.pool import ThreadPool from time import time from db import get_connnection, close_connection, create_trip, insert_trip from parser import parse_trips RUNS_NUMBER = 3 CHUNKS_NUMBER = 4 PAGES_NUMBER = 16 def parse_and_save(urls): parsed_trips = [] for url in urls: next_parsed_trips = parse_trips(url) parsed_trips = [*parsed_trips, *next_parsed_trips] conn = get_connnection() for trip in parsed_trips: trip_to_insert = create_trip(trip) insert_trip(conn, trip_to_insert) close_connection(conn) return 'success' def main(): args = [] base_url = 'https://bolshayastrana.com/tury?plainSearch=1&page=' urls = [base_url + str(i + 1) for i in range(PAGES_NUMBER)] for i in range(CHUNKS_NUMBER): chunk_len = len(urls) / CHUNKS_NUMBER args.append(urls[int(i*chunk_len):int((i+1)*chunk_len)]) pool = ThreadPool(len(args)) start = time() res = pool.map(parse_and_save, args) pool.close() pool.join() exec_time = time() - start return exec_time, res if __name__ == \"__main__\": times = [] for i in range(RUNS_NUMBER): next_time, res = main() times.append(next_time) print(i + 1, ' run: ', next_time, 's. RESULT: ', res) print('Average time: ', sum(times) / len(times), 's') parse_multiprocessing.py import multiprocessing from time import time from db import get_connnection, close_connection, create_trip, insert_trip from parser import parse_trips RUNS_NUMBER = 3 CHUNKS_NUMBER = 4 PAGES_NUMBER = 16 def parse_and_save(urls): parsed_trips = [] for url in urls: next_parsed_trips = parse_trips(url) parsed_trips = [*parsed_trips, *next_parsed_trips] conn = get_connnection() for trip in parsed_trips: trip_to_insert = create_trip(trip) insert_trip(conn, trip_to_insert) close_connection(conn) return 'success' def main(): args = [] base_url = 'https://bolshayastrana.com/tury?plainSearch=1&page=' urls = [base_url + str(i + 1) for i in range(PAGES_NUMBER)] for i in range(CHUNKS_NUMBER): chunk_len = len(urls) / CHUNKS_NUMBER args.append(urls[int(i*chunk_len):int((i+1)*chunk_len)]) pool = multiprocessing.Pool(len(args)) start = time() res = pool.map(parse_and_save, args) pool.close() pool.join() exec_time = time() - start return exec_time, res if __name__ == \"__main__\": times = [] for i in range(RUNS_NUMBER): next_time, res = main() times.append(next_time) print(i + 1, ' run: ', next_time, 's. RESULT: ', res) print('Average time: ', sum(times) / len(times), 's') parse_asyncio.py import asyncio from time import time from db import get_connnection, close_connection, create_trip, insert_trip from parser import parse_trips RUNS_NUMBER = 3 CHUNKS_NUMBER = 4 PAGES_NUMBER = 16 async def parse_and_save(urls): parsed_trips = [] for url in urls: next_parsed_trips = parse_trips(url) parsed_trips = [*parsed_trips, *next_parsed_trips] conn = get_connnection() for trip in parsed_trips: trip_to_insert = create_trip(trip) insert_trip(conn, trip_to_insert) close_connection(conn) return 'success' async def main(): tasks = [] base_url = 'https://bolshayastrana.com/tury?plainSearch=1&page=' urls = [base_url + str(i + 1) for i in range(PAGES_NUMBER)] for i in range(CHUNKS_NUMBER): chunk_len = len(urls) / CHUNKS_NUMBER task = asyncio.create_task(parse_and_save(urls[int(i*chunk_len):int((i+1)*chunk_len)])) tasks.append(task) start = time() res = await asyncio.gather(*tasks) exec_time = time() - start return exec_time, res if __name__ == \"__main__\": times = [] for i in range(RUNS_NUMBER): next_time, res = asyncio.run(main()) times.append(next_time) print(i + 1, ' run: ', next_time, 's. RESULT: ', res) print('Average time: ', sum(times) / len(times), 's') \u0412\u044b\u0432\u043e\u0434\u044b Threading Multiprocessing Asyncio 1 8.34 6.60 8.91 2 8.98 7.11 13.57 3 7.40 7.80 13.35 Avg 8.24 7.17 11.94 Threading : \u041f\u0440\u043e\u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0438\u0440\u043e\u0432\u0430\u043b \u0432\u0440\u0435\u043c\u044f \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f \u0441\u0440\u0430\u0432\u043d\u0438\u043c\u043e\u0435 \u0441 \u043c\u0443\u043b\u044c\u0442\u0438\u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0440\u043d\u043e\u0441\u0442\u044c\u044e. Multiprocessing : \u041f\u043e\u0434\u0445\u043e\u0434 \u043f\u043e\u043a\u0430\u0437\u0430\u043b \u043d\u0430\u0438\u043b\u0443\u0447\u0448\u0438\u0435 \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u0438. \u042d\u0442\u043e \u0441\u0432\u044f\u0437\u0430\u043d\u043e \u0441 \u0440\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u0435\u043c \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0438 \u0437\u0437\u0430\u043f\u0438\u0441\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0430 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0432 Asyncio : \u041d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u043c\u0435\u0434\u043b\u0435\u043d\u043d\u044b\u0439","title":"L2.2"},{"location":"task2/#2","text":"\u041d\u0430\u043f\u0438\u0448\u0438\u0442\u0435 \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0443 \u043d\u0430 Python \u0434\u043b\u044f \u043f\u0430\u0440\u0430\u043b\u043b\u0435\u043b\u044c\u043d\u043e\u0433\u043e \u043f\u0430\u0440\u0441\u0438\u043d\u0433\u0430 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u0445 \u0432\u0435\u0431-\u0441\u0442\u0440\u0430\u043d\u0438\u0446 \u0441 \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0445 \u0432 \u0431\u0430\u0437\u0443 \u0434\u0430\u043d\u043d\u044b\u0445 \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u043f\u043e\u0434\u0445\u043e\u0434\u043e\u0432 threading, multiprocessing \u0438 async. \u041a\u0430\u0436\u0434\u0430\u044f \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0430 \u0434\u043e\u043b\u0436\u043d\u0430 \u043f\u0430\u0440\u0441\u0438\u0442\u044c \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u0441 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u0445 \u0432\u0435\u0431-\u0441\u0430\u0439\u0442\u043e\u0432, \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0442\u044c \u0438\u0445 \u0432 \u0431\u0430\u0437\u0443 \u0434\u0430\u043d\u043d\u044b\u0445.","title":"\u0417\u0430\u0434\u0430\u043d\u0438\u0435 2"},{"location":"task2/#parse_threadingpy","text":"from multiprocessing.pool import ThreadPool from time import time from db import get_connnection, close_connection, create_trip, insert_trip from parser import parse_trips RUNS_NUMBER = 3 CHUNKS_NUMBER = 4 PAGES_NUMBER = 16 def parse_and_save(urls): parsed_trips = [] for url in urls: next_parsed_trips = parse_trips(url) parsed_trips = [*parsed_trips, *next_parsed_trips] conn = get_connnection() for trip in parsed_trips: trip_to_insert = create_trip(trip) insert_trip(conn, trip_to_insert) close_connection(conn) return 'success' def main(): args = [] base_url = 'https://bolshayastrana.com/tury?plainSearch=1&page=' urls = [base_url + str(i + 1) for i in range(PAGES_NUMBER)] for i in range(CHUNKS_NUMBER): chunk_len = len(urls) / CHUNKS_NUMBER args.append(urls[int(i*chunk_len):int((i+1)*chunk_len)]) pool = ThreadPool(len(args)) start = time() res = pool.map(parse_and_save, args) pool.close() pool.join() exec_time = time() - start return exec_time, res if __name__ == \"__main__\": times = [] for i in range(RUNS_NUMBER): next_time, res = main() times.append(next_time) print(i + 1, ' run: ', next_time, 's. RESULT: ', res) print('Average time: ', sum(times) / len(times), 's')","title":"parse_threading.py"},{"location":"task2/#parse_multiprocessingpy","text":"import multiprocessing from time import time from db import get_connnection, close_connection, create_trip, insert_trip from parser import parse_trips RUNS_NUMBER = 3 CHUNKS_NUMBER = 4 PAGES_NUMBER = 16 def parse_and_save(urls): parsed_trips = [] for url in urls: next_parsed_trips = parse_trips(url) parsed_trips = [*parsed_trips, *next_parsed_trips] conn = get_connnection() for trip in parsed_trips: trip_to_insert = create_trip(trip) insert_trip(conn, trip_to_insert) close_connection(conn) return 'success' def main(): args = [] base_url = 'https://bolshayastrana.com/tury?plainSearch=1&page=' urls = [base_url + str(i + 1) for i in range(PAGES_NUMBER)] for i in range(CHUNKS_NUMBER): chunk_len = len(urls) / CHUNKS_NUMBER args.append(urls[int(i*chunk_len):int((i+1)*chunk_len)]) pool = multiprocessing.Pool(len(args)) start = time() res = pool.map(parse_and_save, args) pool.close() pool.join() exec_time = time() - start return exec_time, res if __name__ == \"__main__\": times = [] for i in range(RUNS_NUMBER): next_time, res = main() times.append(next_time) print(i + 1, ' run: ', next_time, 's. RESULT: ', res) print('Average time: ', sum(times) / len(times), 's')","title":"parse_multiprocessing.py"},{"location":"task2/#parse_asynciopy","text":"import asyncio from time import time from db import get_connnection, close_connection, create_trip, insert_trip from parser import parse_trips RUNS_NUMBER = 3 CHUNKS_NUMBER = 4 PAGES_NUMBER = 16 async def parse_and_save(urls): parsed_trips = [] for url in urls: next_parsed_trips = parse_trips(url) parsed_trips = [*parsed_trips, *next_parsed_trips] conn = get_connnection() for trip in parsed_trips: trip_to_insert = create_trip(trip) insert_trip(conn, trip_to_insert) close_connection(conn) return 'success' async def main(): tasks = [] base_url = 'https://bolshayastrana.com/tury?plainSearch=1&page=' urls = [base_url + str(i + 1) for i in range(PAGES_NUMBER)] for i in range(CHUNKS_NUMBER): chunk_len = len(urls) / CHUNKS_NUMBER task = asyncio.create_task(parse_and_save(urls[int(i*chunk_len):int((i+1)*chunk_len)])) tasks.append(task) start = time() res = await asyncio.gather(*tasks) exec_time = time() - start return exec_time, res if __name__ == \"__main__\": times = [] for i in range(RUNS_NUMBER): next_time, res = asyncio.run(main()) times.append(next_time) print(i + 1, ' run: ', next_time, 's. RESULT: ', res) print('Average time: ', sum(times) / len(times), 's')","title":"parse_asyncio.py"},{"location":"task2/#_1","text":"Threading Multiprocessing Asyncio 1 8.34 6.60 8.91 2 8.98 7.11 13.57 3 7.40 7.80 13.35 Avg 8.24 7.17 11.94 Threading : \u041f\u0440\u043e\u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0438\u0440\u043e\u0432\u0430\u043b \u0432\u0440\u0435\u043c\u044f \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f \u0441\u0440\u0430\u0432\u043d\u0438\u043c\u043e\u0435 \u0441 \u043c\u0443\u043b\u044c\u0442\u0438\u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0440\u043d\u043e\u0441\u0442\u044c\u044e. Multiprocessing : \u041f\u043e\u0434\u0445\u043e\u0434 \u043f\u043e\u043a\u0430\u0437\u0430\u043b \u043d\u0430\u0438\u043b\u0443\u0447\u0448\u0438\u0435 \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u0438. \u042d\u0442\u043e \u0441\u0432\u044f\u0437\u0430\u043d\u043e \u0441 \u0440\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u0435\u043c \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0438 \u0437\u0437\u0430\u043f\u0438\u0441\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0430 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0432 Asyncio : \u041d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u043c\u0435\u0434\u043b\u0435\u043d\u043d\u044b\u0439","title":"\u0412\u044b\u0432\u043e\u0434\u044b"}]}